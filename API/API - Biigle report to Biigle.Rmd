---
title: "Processing SMarTaR-ID reports"
author: "Deep Sea Conservation Research Unit - Nils Piechaud & Kerry Howell"
date: "2022"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github

---

This document is to help you convert your annotated data downloaded from Biigle into a format that is suitable for multivariate analysis. It also enables you to link the annotation labels to taxonomic data in the World Register of Marine Species https://www.marinespecies.org/, using their aphia ID numbers.     

## Check that necessary packages have been installed

```{r, message = FALSE}

install.packages( 
setdiff( c("tidyverse","magrittr"),installed.packages()[,1] )
)

library(tidyverse)
library(magrittr)

```

## Set the working environment ## 

This will create the folder needed for the rest of this code to work properly

You can replace the path to the location of the folder you want to work from on your machine (C:/User/Name/Documents/...)
Or leave the code as it is, the first line will pick up the pathway to where your file is


```{r, message=FALSE , warning=FALSE}
# fetch the pathway to the dir where your script is
wd <- dirname(rstudioapi::getSourceEditorContext()$path)

paste0(wd,"/reports") -> reports_dir

 
```

Now that the folders are ready you can manually drop the Biigle CSV reports zips (as many as you want) in the 'reports' folder. 
Instructions to download the csv reports can be found here:
https://biigle.de/manual/tutorials/reports/reports-schema#annotation-csv-report

You need to select the csv type of 'Report variant' in Biigle. 
If you download reports from Biigle at the level of the project, you will need to first unzip the main folder, leaving you with multiple zipped volume folders to the add to the 'reports' folder.

Check that the sheet "Label_tree_aphiaIDs.csv" has successfully downloaded into the 'Taxonomy' folder. This sheet is used to add the aphiaIDs to the smartrID names. If it has successfully installed itself in the folder, all is well, you may continue running code. 

If it is not present in the Taxonomy folder, you will need to manually download from here: https://github.com/DeepSeaCRU/CRU-resources/blob/main/Label_tree_aphiaIDs.csv, and then add to the folder 'Taxonomy'. If the sheet "Label_tree_aphiaIDs.csv" cannot be found, the script will throw an error.

When that is done you can run the rest of the script.


```{r}

# Open all biigle CSV reports in reports_dir

# list the csv tables -
list.files(reports_dir, pattern = "csv_image_annotation_report") -> files

# 2) make a metadata table --------------------------------------------------------------------------------

tibble(file = files) %>%
  mutate(table_name = str_remove(file, pattern = ".zip")) %>%
  # make a column of volume ID number
  mutate(volume = str_remove(table_name, pattern = "_csv_image_annotation_report")) -> annotationTables


# make a list to store all transformed tables tables
Dframes  <- as.list(1:nrow(annotationTables))

for (i in seq(Dframes)) {
  # select the table number i
  annotationTables %>% slice(i) -> meta.i
  # import it
  meta.i %>% pull(file) %>% paste(reports_dir, ., sep = "/") %>%
    read_csv(col_types = cols()) -> D.i
  # add the metadata
  bind_cols(D.i, meta.i) ->  Dframes[[i]]
  
}

# This is your table of everything
Dframes %>% bind_rows() -> allannotations


allannotations %>% 
  filter(filename %in% c("M116_13331848_13109013355679.jpg",
                         "M116_13331848_13109013510676.jpg",
                         "M116_13331848_13109013512676.jpg")) -> annotations



```


## biigle API parameters

To interact with Biigle, you can either use the manual interface or the
API. since you already know where your annotations are in the image and
what labels they have you can use the API. You will need to know you
Biigle credential before you are authorized to interact with your
volumes. You also need a token for your identity to be tied to your
requests and authorized. You can get your token at:
<https://biigle.de/manual/tutorials/login-and-account-settings>

```{r}

# put your credentials here: 
# login <- "your email address"
# token <- "your token number"


# base url
base_url <- "https://biigle.de/api/v1"

# the ID of the volume you will add your images to  
Volume <- 8188 # ID code of the volume to modify


# go check your volume
shell.exec(paste0("https://biigle.de/volumes/",Volume))

```

Once you are sure where your images will go, we need to get some info
from biigle about how it calls these images so we can give it the
instructions it understands

first, get the ids of the images in r. This is also a gentle intro to
using the API When you send a request to the server it respond with a
code that means success, fail or inbetween

Some server responses Status: 201 \# Created -- that probably means
success Status: 422 \# unprocessable entity -- I guess that is a fail

all API commands are listed here:
<https://biigle.de/doc/api/index.html#api-_>

this step can take a while if there are a lot of images to get the info
about


To make sure Image names and their ID are lined-up properly, We first download the list of IDs from the Biigle volume, them download the filename associated to each ID. that way, you can ensure correct match between image name and ID even if some images in the volume do not have annotations

this takes some time but is necessary to keep some flexibility in the script

```{r}


# make a list of images where you have annotations
annotations %>% distinct(filename) %>% pull() -> imgs


# get the new images infos (ID etc)
      # build the URL from the base API address, which volu
      url <- paste0(base_url, 
                    # which volume you want info from
                    "/volumes/",Volume,
                    # and what exacly you want to know about this volums
                    "/files" )

# it should look like this : "https://biigle.de/api/v1/volumes/123456789/files"

      
# Then, send the request to get a list of image IDs for that volume
      # send the request with you credential attached
      GET(url, authenticate(login, token) ) %>% 
        content(., as = "text", encoding = "UTF-8") %>% 
        jsonlite :: fromJSON( flatten = TRUE) -> imageIDs
      
# get the info for each image

  Image_infos <- list()
  # monitor server response 
  rps <- list()
  
  for(i in seq(imageIDs)){
    
    paste0( (i / length(imageIDs )) *100, " % of the images" ) %>% print()
    
        url.i <- paste0(base_url,"/images/", imageIDs[i] )
        GET(url.i, authenticate(login, token) ) -> rp
        
        rp %>% 
          content(., as = "text", encoding = "UTF-8") %>% 
          jsonlite :: fromJSON( flatten = TRUE) %>%
          unlist %>% t %>% as.data.frame() -> Image_infos[[i]]
        
        print(paste0(Image_infos[[i]]$filename))
        print(Image_infos[[i]]$id)
        
        rp -> rps[[i]]
    
  }
      

  Image_infos %>% bind_rows() %>% as_tibble() %>%
     mutate(attrs.width = as.numeric(attrs.width), attrs.height = as.numeric(attrs.height)) %>% 
    select(biigleid = id, filename, volume_id,attrs.width,attrs.height) -> imageinfos   
  
    
      
# tibble(filename = imgs, biigleid = imageIDs, volume_id = Volume) -> imageinfos

      
# get the new image IDs in the annotation file - merge with teh actual name of the images
# Images that have annotations but are not on the Biigle volume will be ignored
imageinfos %<>%  filter(filename %in% annotations$filename ) 

# attach the image infor to the annotations list
imageinfos  %>%
# join with the annotations so you can link images to their new ID in the new volume
  left_join(annotations,by = "filename" , keep = FALSE ) %>% 
    select(biigleid  , filename, volume_id,  
           # Essential column the API needs to make new annotations
           # ID of the label name 
           label_id, label_name, 
           # shape (rectangle, circle, point... etc)
           shape_id, shape_name,
           # a string of coordinates corresponding to the said shape location
           points )  -> R_annotations

  
 
```

last but important, add some additional parameters that the API can
interpret, the shape of the annotation is most important

```{r}

# add columns for shapes and confidence
annotations_upload <- R_annotations %>% 
  # remove the unnecessary columns
  # add the arguments that the API will interpret to make the labels
  mutate( confidence = 1) 


# apply filter here
annotations_upload %>%  filter()

M116_13331848_13109013355679.jpg

# apply labels changes here

 
 
```


## Uploading the annotations

You should be all set to actually start uploading the images

it is good practice to run the loop once (i = 1) to see if the label
gets in the right place. Run the commands once, it should print what
image and label it used. Go to this image on your volume, there should
now be one annotation. If it looks right, delete it manually (or it will
get duplicated) and run the loop.

You may want to run the loop without actually send the requests to chek
the URLs are correctly assembled. in which case, dont give it the green
light.

by design, you can only send 10,800 requests per hour. There are a
couple of ways around this but they are not covered here.

the rp list collects the server response to your request. If all goes
well, it will say "201". other codes exists for when things dont work.
This is not always helpful but can help narrowing down what the issue
is.

```{r}

# upload -----------------------------------------------------------------------
# are you sure want to upload? only give green light if you are certain the code is right 
# errors could have unexpected consequences
upload_enabled <- "Green" # "Green" or "notGreen

# make sure there are not too many annotations 
if( annotations_upload %>% nrow() > 10799){print("Too many annotations, this will not gor through")}
 
# server responses to annotation input - - - - - - - - - - - - - -- - - - - -  -
upload_response <- list()

for (i in 1:nrow(annotations_upload)) {
  
  annotations_upload %>% slice(i) -> A.I
  cat( paste( A.I$filename," (id:", A.I$biigleid, ") - - - " , A.I$label_name), "in" , A.I$shape_name)
   
 
  paste0(base_url,"/images/", A.I$biigleid,"/annotations" ) -> url.i
  # make the query list for each annotation
  # these are the required parameters
  list( shape_id = A.I$shape_id, # 4 is for circle
        label_id = A.I$label_id,
        # add confidence number - Either the one from yolo
        confidence = A.I$confidence,
        # or, an arbitrary number
        # confidence = 0.75 ,
        # make the list of points coordinates
        points = A.I$points  %>% 
          str_sub(start = 2,end = -2) %>% # remove the []
          str_split(",") %>%
          unlist %>% # make a numerical vector
          as.numeric()) -> body_list
  
  
  # send the request (if enabled)
  if(upload_enabled == "Green"){
    cat("Sending request to server - - ")
    # say that to the API
    POST(url.i,
         config = authenticate(login, token),
         body = body_list, 
         encode = "json"    ) -> rp
    # example image
    rp -> upload_response[[length(upload_response) +1]]
  }# 
  print("Next annotation") 
  rm(url.i)
  
}      
      


```

